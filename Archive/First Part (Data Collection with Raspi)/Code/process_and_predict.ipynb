{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def logger(input_string: str) -> None:\n",
    "    file_path = \"output/log.txt\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(input_string + \"\\n\")\n",
    "    else:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(input_string + \"\\n\")\n",
    "    print(\"Logger Message: \" + input_string)\n",
    "\n",
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "logger(\"\\n\\n_________________________________________________________________\")\n",
    "logger(\"New script run: \" + str(time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(\"data_raw.csv\")\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First row is named \"data\" is the date in YYYYMMMDD so we format to daytime then sort the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data[\"date\"] = pd.to_datetime(data[data.columns[0]], format=\"%Y%m%d\")\n",
    "    data.drop(data.columns[0], axis=1, inplace=True)\n",
    "    data.sort_values(by='date', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On weekend the exchange is closed so we need to fill the missing values with linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    min_date = data[\"date\"].min()\n",
    "    max_date = data[\"date\"].max()\n",
    "    all_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "    todo_data = [\"date_global_quote\",\"open_price\",\"high_price\",\"low_price\",\"closing_price\",\"volume\"]\n",
    "    data_helper = data[todo_data].copy()\n",
    "    #get a list with only the dates we have data for\n",
    "    data_helper[\"date_global_quote\"] = pd.to_datetime(data_helper[\"date_global_quote\"])\n",
    "    data_helper.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    #merge the list of all dates with the data we have\n",
    "    data_with_missing_dates = pd.merge(pd.DataFrame({\"date_global_quote\": all_dates}), data_helper, on=\"date_global_quote\", how=\"left\")\n",
    "    data_with_missing_dates.sort_values(by=\"date_global_quote\", inplace=True) \n",
    "    data_with_missing_dates.interpolate(method='linear', inplace=True)\n",
    "\n",
    "    data[todo_data] = data_with_missing_dates[todo_data]\n",
    "    \n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add days of week in numbers 0-6, add a mean price, a price change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data['weekday'] = data['date'].dt.dayofweek\n",
    "\n",
    "    data['mean_price'] = data[['open_price', 'low_price', 'high_price', 'closing_price']].mean(axis=1)\n",
    "\n",
    "    data[\"next_day_percentage\"] = (data[\"mean_price\"].shift(-1) / data[\"mean_price\"] - 1)*100\n",
    "    data.fillna({'next_day_percentage': 0}, inplace=True)\n",
    "\n",
    "    data[\"price_change\"] = data[\"mean_price\"] / data[\"mean_price\"].shift(1)\n",
    "    data.fillna({'price_change': 1}, inplace=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert title list to numbers using finbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the needed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "    if not os.path.exists('.venv/Transformer'):\n",
    "        finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "        finbert.save_pretrained('.venv/Transformer/Finbert_Offline')\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "        tokenizer.save_pretrained('.venv/Transformer/Tokenizer_Offline')\n",
    "        logger(\"Transfomers were downloaded because no folder was found\")\n",
    "    else: logger(\".venv/Transformer were found and reused offline\")\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "try:\n",
    "    finbert = BertForSequenceClassification.from_pretrained('.venv/Transformer/Finbert_Offline',num_labels=3)\n",
    "    tokenizer = BertTokenizer.from_pretrained('.venv/Transformer/Tokenizer_Offline')\n",
    "    \n",
    "    nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "    def finbert(input_string):\n",
    "        # Return 0 immediately if input is \"0\"\n",
    "        if input_string == \"0\":\n",
    "            return 0\n",
    "\n",
    "        text_list = ast.literal_eval(input_string)  # Convert string to list\n",
    "        if not text_list:\n",
    "            return 0\n",
    "\n",
    "        scores = []\n",
    "        for text in text_list:\n",
    "            # Truncate text to fit tokenization limit\n",
    "            while len(tokenizer.tokenize(text)) > 500:\n",
    "                text = text[:-1]\n",
    "\n",
    "            # Analyze text with finbert\n",
    "            results = nlp(text)\n",
    "            for item in results:\n",
    "                score = item['score']\n",
    "                label = item['label']\n",
    "                # Convert label to numerical value and multiply by score\n",
    "                if label == \"Neutral\":\n",
    "                    scores.append(0)\n",
    "                elif label == \"Negative\":\n",
    "                    scores.append(-10 * score)\n",
    "                elif label == \"Positive\":\n",
    "                    scores.append(10 * score)\n",
    "        # Calculate mean if scores list is not empty\n",
    "        return sum(scores) / len(scores) if scores else 0\n",
    "\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_finbert = pd.DataFrame()\n",
    "    data_finbert[[\"com_title_finbert\", \"ceo_title_finbert\"]] = data[[\"com_title_list\", \"ceo_title_list\"]].apply(lambda col: col.map(finbert))\n",
    "    logger(\"Finbert was applied to the data\")\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Finbert data. Data can be reuses like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    data[[\"com_title_finbert\",\"ceo_title_finbert\"]] = data_finbert[[\"com_title_finbert\",\"ceo_title_finbert\"]]\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currency exchange is the only thing live so we need to adjust the date because everything else is from yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"currency_exchange_rate\"] = data[\"currency_exchange_rate\"].shift(1)\n",
    "data[\"currency_exchange_rate\"].bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the columns that are a date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0.0, inplace=True)\n",
    "data_no_dates = data.drop(columns=data.filter(like='date').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if columns are numbers or text or dont change to much and then delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    features = []\n",
    "    needed_unique = 5\n",
    "\n",
    "    for column in data_no_dates.columns[1:]:#sorts the columns in number, text, norchanging\n",
    "        if pd.to_numeric(data_no_dates[column], errors='coerce').notnull().all():# Check if the column is numeric\n",
    "            data_no_dates[column] = pd.to_numeric(data_no_dates[column])# Convert to number\n",
    "            if data_no_dates[column].nunique() >= needed_unique:#check if all calues are the same. if so we dont need them    \n",
    "                features.append(column)\n",
    "\n",
    "    data_features = data_no_dates[features]\n",
    "    logger(\"Features: \" + str(features))\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data by subtracting the mean and dividing by the standard deviation (Z-Score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_normalized = data_features.sub(data_features.mean(axis=0), axis=1).div((data_features.max(axis=0)-data_features.min(axis=0)), axis=1)\n",
    "    logger(\"Data normalized with Z-Score\")\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the target column which is the next days price change in percentage. So it the price will rise 5% the next day, the target will be 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all features in one plot if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "for feature in features:\n",
    "    plt.plot(data['date'], data_normalized[feature])\n",
    "plt.title('Features')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "every_fifth_date = data['date'][::5]\n",
    "plt.xticks(every_fifth_date, rotation=90)\n",
    "plt.savefig('output/Features.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    target = data['next_day_percentage']\n",
    "    features = data_normalized.drop(['next_day_percentage'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of ML Models that are then tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"Linear Regression\",\n",
    "        \"model\": LinearRegression(),\n",
    "        \"param_grid\": {}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Decision Tree\",\n",
    "        \"model\": DecisionTreeRegressor(),\n",
    "        \"param_grid\": {}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model\": RandomForestRegressor(),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200, 300, 1000],\n",
    "            'max_features': [None, 'sqrt', 'log2'],\n",
    "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gradient Boosting\",\n",
    "        \"model\": GradientBoostingRegressor(),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200, 300, 1000],\n",
    "            'max_features': [None, 'sqrt', 'log2'],\n",
    "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVR\",\n",
    "        \"model\": SVR(),\n",
    "        \"param_grid\": {\n",
    "            'C': [0.1, 1, 10, 100, 1000],\n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KNN\",\n",
    "        \"model\": KNeighborsRegressor(),\n",
    "        \"param_grid\": {\n",
    "            'n_neighbors': [3, 5, 11, 19],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Neural Network\",\n",
    "        \"model\": MLPRegressor(),\n",
    "        \"param_grid\": {\n",
    "            'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 25)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam', 'lbfgs'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "            'learning_rate': ['constant', 'adaptive']\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        \n",
    "    from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    best_model = None\n",
    "\n",
    "    for model_info in models:\n",
    "        name = model_info[\"name\"]\n",
    "        model = model_info[\"model\"]\n",
    "        param_grid = model_info[\"param_grid\"]\n",
    "        \n",
    "        estimator = model\n",
    "        kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "        random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, cv=kfold, n_jobs=-1, verbose=2)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        model = model.__class__(**random_search.best_params_)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        logger(\"Model:\" + name + \", Mean Squared Error:\" + str(mse))\n",
    "        \n",
    "        if best_model is None or mse < best_model[\"mse\"]:\n",
    "            best_model = {\n",
    "                \"name\": name,\n",
    "                \"model\": model,\n",
    "                \"mse\": mse\n",
    "            }\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model[\"model\"]\n",
    "mse = best_model[\"mse\"]\n",
    "logger(\"Best model: \" + str(model) + \" Mean Squared Error: \" + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"next_day_percentage_predicted\"] = model.predict(features)\n",
    "data[\"diff_percentage\"] = abs((data[\"next_day_percentage\"] - data[\"next_day_percentage_predicted\"]) / data[\"next_day_percentage\"]) * 100\n",
    "mean_diff = data[\"diff_percentage\"][:-1].mean()\n",
    "\n",
    "logger(\"Mean Difference between predicted and real is: \" + str(mean_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the next days price change in percent vs the predicted values from the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(data['date'], data[[\"next_day_percentage\", \"next_day_percentage_predicted\"]])\n",
    "plt.title('Next Days Percentage\\nMean Diff: ' + str(mean_diff))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "every_fifth_date = data['date'][::5]\n",
    "plt.xticks(every_fifth_date, rotation=90)\n",
    "plt.savefig('output/Predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the best threshold of the data to make the best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "iterations = 1000\n",
    "stock_value = 1000\n",
    "money_value = 0\n",
    "fee = 1\n",
    "sell_percentage = 0.5\n",
    "data[\"depot_value\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_loop(trial, data, stock_value, money_value, sell_percentage):\n",
    "    buy_threshold = trial.suggest_float('buy_threshold', 0, 5)\n",
    "    sell_threshold = trial.suggest_float('sell_threshold', -5, 0)\n",
    "\n",
    "    data[\"depot_value\"] =  broker(stock_value, money_value, buy_threshold, sell_threshold, sell_percentage, data)\n",
    "    last_depot_value = data['depot_value'].iloc[-1]\n",
    "    return last_depot_value\n",
    "\n",
    "def broker(stock_value, money_value, buy_threshold, sell_threshold, sell_percentage, data):\n",
    "    stock_value = stock_value\n",
    "    money_value = money_value\n",
    "    depot_value = 0\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        stock_value *= row[\"price_change\"]\n",
    "        if row['next_day_percentage_predicted'] >= buy_threshold:\n",
    "            if money_value > 0:\n",
    "                money_value -= fee\n",
    "                stock_value += money_value\n",
    "                money_value = 0\n",
    "        elif row['next_day_percentage_predicted'] <= sell_threshold:\n",
    "                if stock_value > 0:\n",
    "                    money_value += stock_value * sell_percentage\n",
    "                    stock_value -= stock_value * sell_percentage\n",
    "                    money_value -= fee\n",
    "        depot_value = stock_value + money_value\n",
    "        data.at[index, 'depot_value'] = depot_value\n",
    "    return data[\"depot_value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    for _ in range(iterations):\n",
    "        study.optimize(lambda trial: optuna_loop(trial, data, stock_value, money_value, sell_percentage), n_trials=1)\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "buy_threshold = round(best_params['buy_threshold'],3)\n",
    "sell_threshold = round(best_params['sell_threshold'],3)\n",
    "stock_value = 1000\n",
    "money_value = 0\n",
    "logger(\"Buy threshold: \" + str(buy_threshold) + \"%, Sell threshold: \" + str(sell_threshold)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    data[\"buy_or_sell\"] = data[\"next_day_percentage_predicted\"].apply(lambda x: 1 if x >= buy_threshold else (-1 if x <= sell_threshold else 0))\n",
    "    data[\"depot_value\"] = broker(stock_value, money_value, buy_threshold, sell_threshold, sell_percentage, data)\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_percentage = round(((data['mean_price'].iloc[-1] / data['mean_price'].iloc[0]) - 1) * 100, 2)\n",
    "spai_percentage = round(((data['depot_value'].iloc[-1] / data['depot_value'].iloc[0]) - 1) * 100, 2)\n",
    "\n",
    "logger(\"Hold percentage: \" + str(hold_percentage) + \"%, SPAI percentage: \" + str(spai_percentage)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bring them to the same level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mean_price'] = data['mean_price'] / data['mean_price'].iloc[0]\n",
    "data['depot_value'] = data['depot_value'] / data['depot_value'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get last value of buy_or_sell\n",
    "today_buy_or_sell = data['buy_or_sell'].iloc[-1]\n",
    "today = data['date'].iloc[-1]\n",
    "\n",
    "if today_buy_or_sell == 1:\n",
    "    print(\"Buy signal today \", today)\n",
    "elif today_buy_or_sell == -1:\n",
    "    print(\"Sell signal today\", today)\n",
    "elif today_buy_or_sell == 0:\n",
    "    print(\"Hold signal today\", today)\n",
    "\n",
    "logger(\"Signal for today: \" + str(today_buy_or_sell) + \", Date: \" + str(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"Hold: {hold_percentage}% vs. SPAI: {spai_percentage}% \\n Buy threshold: {buy_threshold}, Sell threshold: {sell_threshold})\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(data['date'], data[\"mean_price\"], label='Holding', marker='o', color='blue')\n",
    "plt.plot(data['date'], data[\"depot_value\"], label='Using SPAI', marker='x', color='purple')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['buy_or_sell'] >= 1:\n",
    "        plt.bar(row['date'], height=0.1, bottom=row[\"mean_price\"] - 0.05, color='green', width=1, alpha=0.5)\n",
    "    elif row['buy_or_sell'] <= -1:\n",
    "        plt.bar(row['date'], height=0.1, bottom=row[\"mean_price\"] - 0.05, color='red', width=1, alpha=0.5)\n",
    "plt.title(title)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "every_fifth_date = data['date'][::5]\n",
    "plt.xticks(every_fifth_date, rotation=90)\n",
    "plt.savefig('output/Simulation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to Output folder\n",
    "data.to_csv('output/data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = pd.Timestamp.now()\n",
    "time_delta = time_end - time_start\n",
    "logger(\"Script ended: \" + str(time_start))\n",
    "logger(\"Script needed: \" + str(time_delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
