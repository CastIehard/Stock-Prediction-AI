{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definde a logger method to log the messages to another file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def logger(input_string: str) -> None:\n",
    "    file_path = \"log.txt\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(input_string + \"\\n\")\n",
    "    else:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(input_string + \"\\n\")\n",
    "    print(\"Logger message: \" + input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Script by adding a message to the logger file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger message: \n",
      "\n",
      "_________________________________________________________________\n",
      "Logger message: New script run: 2024-07-02 20:46:55.051630\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "logger(\"\\n\\n_________________________________________________________________\")\n",
    "logger(\"New script run: \" + str(time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the raw data and convert it to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert first column to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"] = pd.to_datetime(data[data.columns[0]], format=\"%Y%m%d\")\n",
    "data.drop(data.columns[0], axis=1, inplace=True)\n",
    "data.sort_values(by='date', inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On weekend the exchange is closed so we need to fill the missing values with linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_date = data[\"date\"].min()\n",
    "# max_date = data[\"date\"].max()\n",
    "# all_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# todo_data = [\"date_global_quote\",\"open_price\",\"high_price\",\"low_price\",\"closing_price\",\"volume\"]\n",
    "# data_helper = data[todo_data].copy()\n",
    "# #get a list with only the dates we have data for\n",
    "# data_helper[\"date_global_quote\"] = pd.to_datetime(data_helper[\"date_global_quote\"])\n",
    "# data_helper.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# #merge the list of all dates with the data we have\n",
    "# data_with_missing_dates = pd.merge(pd.DataFrame({\"date_global_quote\": all_dates}), data_helper, on=\"date_global_quote\", how=\"left\")\n",
    "# data_with_missing_dates.sort_values(by=\"date_global_quote\", inplace=True) \n",
    "# data_with_missing_dates.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# data[todo_data] = data_with_missing_dates[todo_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the target, which is the percentage the stock price will increase or decrease the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mean_price'] = data[['open_price', 'low_price', 'high_price', 'closing_price']].mean(axis=1)\n",
    "\n",
    "data[\"next_day_percentage\"] = (data[\"mean_price\"].shift(-1) / data[\"mean_price\"] - 1)*100\n",
    "data.fillna({'next_day_percentage': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday'] = data['date'].dt.dayofweek #add weekday column to the dataframe\n",
    "\n",
    "data[\"price_change_1\"] = data[\"mean_price\"] / data[\"mean_price\"].shift(1)#calculate the price change last day to current day\n",
    "data.fillna({'price_change_1': 1}, inplace=True)\n",
    "\n",
    "data[\"price_change_3\"] = data[\"mean_price\"] / data[\"mean_price\"].shift(3)#calculate the price change 3 rdlast day to current day\n",
    "data.fillna({'price_change_1': 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the needed Finbert model and tokenizer if not already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger message: Finbert model loaded\n"
     ]
    }
   ],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "finbert.save_pretrained('Transformer/Finbert_Offline')\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "tokenizer.save_pretrained('Transformer/Tokenizer_Offline')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "logger(\"Finbert model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finbert function that takes in string and returns the sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast #for converting string to list\n",
    "def finbert(input_string):\n",
    "    # Return 0 immediately if input is \"0\"\n",
    "    if input_string == \"0\":\n",
    "        return 0\n",
    "\n",
    "    text_list = ast.literal_eval(input_string)  # Convert string to list\n",
    "    if not text_list:\n",
    "        return 0\n",
    "\n",
    "    scores = []\n",
    "    for text in text_list:\n",
    "        # Truncate text to fit tokenization limit\n",
    "        while len(tokenizer.tokenize(text)) > 500:\n",
    "            text = text[:-1]\n",
    "\n",
    "        # Analyze text with finbert\n",
    "        results = nlp(text)\n",
    "        for item in results:\n",
    "            score = item['score']\n",
    "            label = item['label']\n",
    "            # Convert label to numerical value and multiply by score\n",
    "            if label == \"Neutral\":\n",
    "                scores.append(0)\n",
    "            elif label == \"Negative\":\n",
    "                scores.append(-1 * score)\n",
    "            elif label == \"Positive\":\n",
    "                scores.append(1 * score)\n",
    "    score = sum(scores) / len(scores) if scores else 0\n",
    "    print(str(score) + \" --> \" + str(text_list))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger message: Finbert data already exists in the data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_check = [\"com_title_finbert\", \"ceo_title_finbert\"]\n",
    "columns_exist = all(column in data.columns for column in columns_to_check)\n",
    "\n",
    "if not columns_exist: #if the columns not already exists\n",
    "    local_time_start = pd.Timestamp.now()\n",
    "    data_finbert = pd.DataFrame()\n",
    "    data_finbert[[\"com_title_finbert\", \"ceo_title_finbert\"]] = data[[\"com_title_list\", \"ceo_title_list\"]].apply(lambda col: col.map(finbert))\n",
    "    local_time_end = pd.Timestamp.now()\n",
    "    time_delta = local_time_end - local_time_start\n",
    "    logger(\"Finbert was applied to the data it took \"+ str(time_delta))\n",
    "    \n",
    "    data[[\"com_title_finbert\",\"ceo_title_finbert\"]] = data_finbert[[\"com_title_finbert\",\"ceo_title_finbert\"]]\n",
    "\n",
    "    # Read the CSV file\n",
    "    data_raw = pd.read_csv('data_raw.csv')\n",
    "    # Add the two columns\n",
    "    data_raw['com_title_finbert'] = data['com_title_finbert']\n",
    "    data_raw['ceo_title_finbert'] = data['ceo_title_finbert']\n",
    "    # Close the file\n",
    "    data_raw.to_csv('data_raw.csv', index=False)\n",
    "    logger(\"Finbert data added to the raw data\")\n",
    "else:\n",
    "    logger(\"Finbert data already exists in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0.0, inplace=True)\n",
    "data_no_dates = data.drop(columns=data.filter(like='date').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the parameters, that have several unique values, so we can use them to predict the target. Also we cant use text parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger message: Features: ['ceo_news_amount', 'alpha_news_amount', 'alpha_news_sentiment_mean', 'open_price', 'high_price', 'low_price', 'closing_price', 'volume', 'currency_exchange_rate', 'cpi', 'eps', 'gdp', 'retail_sales', 'market_capitalization', 'pe_ratio', 'peg_ratio', 'eps.1', 'diluted_eps_ttm', 'analyst_target_price', 'trailing_pe', 'forward_pe', 'price_to_sales_ratio_ttm', 'price_to_book_ratio', 'ev_to_revenue', 'ev_to_ebitda', 'beta', 'week_high_52', 'week_low_52', 'day_moving_average_50', 'day_moving_average_200', 'otherCurrentAssets', 'com_title_finbert', 'ceo_title_finbert', 'mean_price', 'next_day_percentage', 'weekday', 'price_change_1', 'price_change_3']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "needed_unique = 5\n",
    "\n",
    "try:\n",
    "    for column in data_no_dates.columns[1:]:#sorts the columns in number, text, norchanging\n",
    "        if pd.to_numeric(data_no_dates[column], errors='coerce').notnull().all():# Check if the column is numeric\n",
    "            data_no_dates[column] = pd.to_numeric(data_no_dates[column])# Convert to number\n",
    "            if data_no_dates[column].nunique() >= needed_unique:#check if all calues are the same. if so we dont need them    \n",
    "                features.append(column)\n",
    "    data_features = data_no_dates[features]\n",
    "    logger(\"Features: \" + str(features))\n",
    "except Exception as e:\n",
    "    logger(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data by subtracting the mean and dividing by the standard deviation (Z-Score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data_features.drop(['next_day_percentage'], axis=1)\n",
    "data_normalized = data_features.sub(data_features.mean(axis=0), axis=1).div((data_features.max(axis=0)-data_features.min(axis=0)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistics_csv(original_df, output_csv_path):\n",
    "    statistics = {\n",
    "        'Column Name': [],\n",
    "        'Mean': [],\n",
    "        'Max': [],\n",
    "        'Min': []\n",
    "    }\n",
    "\n",
    "    for column in original_df.columns:\n",
    "        statistics['Column Name'].append(column)\n",
    "        statistics['Mean'].append(original_df[column].mean())\n",
    "        statistics['Max'].append(original_df[column].max())\n",
    "        statistics['Min'].append(original_df[column].min())\n",
    "\n",
    "    statistics_df = pd.DataFrame(statistics)\n",
    "    statistics_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statistics_csv(data_features, 'normalizaton_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA- Principal Component Analysis to reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components='mle',svd_solver = 'full')\n",
    "# data_pca = pca.fit_transform(data_normalized)\n",
    "# #convert to dataframe\n",
    "# data_pca = pd.DataFrame(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "for column in data_normalized.columns:\n",
    "    plt.plot(data['date'], data_normalized[column])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(data['date'][::10], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data in training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target = data['next_day_percentage'] #use the one from data so its not normalized\n",
    "features = data_features\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list with all the Models and their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"Linear Regression\",\n",
    "        \"model\": LinearRegression(),\n",
    "        \"param_grid\": {}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Decision Tree\",\n",
    "        \"model\": DecisionTreeRegressor(),\n",
    "        \"param_grid\": {}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model\": RandomForestRegressor(),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200, 300, 1000],\n",
    "            'max_features': [None, 'sqrt', 'log2'],\n",
    "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gradient Boosting\",\n",
    "        \"model\": GradientBoostingRegressor(),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100,500,1000],\n",
    "            'max_features': [None, 'sqrt', 'log2'],\n",
    "            'max_depth': [10,50,100, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'learning_rate': [0.01,0.1,0.5]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KNN\",\n",
    "        \"model\": KNeighborsRegressor(),\n",
    "        \"param_grid\": {\n",
    "            'n_neighbors': [3, 5, 11, 19],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Neural Network\",\n",
    "        \"model\": MLPRegressor(max_iter=10000),\n",
    "        \"param_grid\": {\n",
    "            'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 25)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam', 'lbfgs'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "            'learning_rate': ['constant', 'adaptive']\n",
    "        }\n",
    "     },\n",
    "    {\n",
    "        \"name\": \"SVR\",\n",
    "        \"model\": SVR(),\n",
    "        \"param_grid\": {\n",
    "            'C': [0.1, 1, 10, 100, 1000],\n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_models_to_csv(mse_list,r2_list,combined_list):\n",
    "    # Open the modells.csv file in append mode\n",
    "    with open('output/modells_mse.csv', 'a', newline='') as file:\n",
    "        # Add the mse_list to the csv file\n",
    "        csv_writer_mse = csv.writer(file)\n",
    "        csv_writer_mse.writerow(mse_list)\n",
    "    \n",
    "    # Open the modells.csv file in append mode\n",
    "    with open('output/modells_r2.csv', 'a', newline='') as file:\n",
    "        # Add the r2_list to the csv file\n",
    "        csv_writer_r2 = csv.writer(file)\n",
    "        csv_writer_r2.writerow(r2_list)\n",
    "\n",
    "    # Open the modells.csv file in append mode\n",
    "    with open('output/modells_together.csv', 'a', newline='') as file:\n",
    "        # Add the combined to the csv file\n",
    "        csv_writer_r2 = csv.writer(file)\n",
    "        csv_writer_r2.writerow(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "description = 'nix'#input(\"Enter a description for this run: \")\n",
    "\n",
    "logger(\"Description: \" + description)\n",
    "mse_list = []\n",
    "r2_list = []\n",
    "combined_list = []\n",
    "\n",
    "mse_list.append(today)\n",
    "mse_list.append(description)\n",
    "r2_list.append(today)\n",
    "r2_list.append(description)\n",
    "combined_list.append(today)\n",
    "combined_list.append(description)\n",
    "\n",
    "best_model = None\n",
    "\n",
    "for model_info in models:#iterate through the models, predict and save the mse and r2\n",
    "    local_time_start = pd.Timestamp.now()\n",
    "    name = model_info[\"name\"]\n",
    "    model = model_info[\"model\"]\n",
    "    param_grid = model_info[\"param_grid\"]\n",
    "    \n",
    "    estimator = model\n",
    "    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, cv=kfold, n_jobs=-1, verbose=0)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    model = model.__class__(**random_search.best_params_)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "    combined_metric = (1 - r2) * mse\n",
    "    combined_list.append(combined_metric)\n",
    "    \n",
    "    local_time_end = pd.Timestamp.now()\n",
    "    time_delta = local_time_end - local_time_start\n",
    "    logger(\"Model:\" + name + \", Mean Squared Error:\" + str(mse) + \", R2 Score:\" + str(r2) + \" Time taken: \" + str(time_delta))\n",
    "    if best_model is None or combined_metric < best_model[\"score\"]:\n",
    "        best_model = {\n",
    "            \"name\": name,\n",
    "            \"model\": model,\n",
    "            \"score\": combined_metric\n",
    "        }\n",
    "\n",
    "save_models_to_csv(mse_list, r2_list, combined_list)\n",
    "logger(\"Models saved to modells.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model[\"model\"]\n",
    "score = best_model[\"score\"]\n",
    "logger(\"Best model: \" + str(model) + \" Mean Squared Error: \" + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Values with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"next_day_percentage_predicted\"] = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(data['date'], data[[\"next_day_percentage\", \"next_day_percentage_predicted\"]])\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Preisänderung zum nächsten Tag in %')\n",
    "plt.legend([\"Tatsächlich\", \"Vorhersage\"])\n",
    "\n",
    "plt.xticks(data['date'][::10], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Predictions.pdf', format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_table(benchmark):\n",
    "    df = pd.read_csv(f'output/modells_{benchmark}.csv')\n",
    "    df.drop(['Date', 'Description'], axis=1, inplace=True)\n",
    "\n",
    "    # Calculate mean value between all values of one row\n",
    "    df['Mean'] = df.mean(axis=1)\n",
    "    df['Improvement'] = df['Mean'] - df['Mean'].shift(1)\n",
    "\n",
    "    # Create a colormap and reverse it based on the benchmark for main data\n",
    "    if benchmark == 'r2':\n",
    "        cmap = sns.color_palette(\"coolwarm_r\", as_cmap=True)\n",
    "    else:\n",
    "        cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    \n",
    "    # Prepare 'Improvement' column for white background\n",
    "    improvement_data = df['Improvement'].copy()\n",
    "    df['Improvement'] = np.nan  # Set to NaN to mask and use default background (white)\n",
    "\n",
    "    # Plot the main dataframe\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df, annot=True, cmap=cmap, fmt=\".2f\", cbar=False)\n",
    "    \n",
    "    # Add the 'Improvement' column text manually with default background\n",
    "    for i in range(len(improvement_data)):\n",
    "        plt.text(df.columns.get_loc('Improvement') + 0.5, i + 0.5, f'{improvement_data.iloc[i]:.2f}',\n",
    "                 ha='center', va='center')\n",
    "\n",
    "    plt.title(f\"Model Performance {benchmark}\")\n",
    "    plt.xticks(rotation=45)  # Rotate column labels\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output/colored_table_{benchmark}.pdf\", format=\"pdf\")\n",
    "    \n",
    "\n",
    "plot_table('mse')\n",
    "plot_table('r2')\n",
    "plot_table('together')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the best threshold of the data to make the best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "iterations = 1000\n",
    "stock_value = 500\n",
    "money_value = 500\n",
    "fee = 1\n",
    "sell_percentage = 0.5\n",
    "data[\"depot_value\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_loop(trial, data, stock_value, money_value, sell_percentage):\n",
    "    buy_threshold = trial.suggest_float('buy_threshold', 0, 10)\n",
    "    sell_threshold = trial.suggest_float('sell_threshold', -10, 0)\n",
    "\n",
    "    data[\"depot_value\"] =  broker(stock_value, money_value, buy_threshold, sell_threshold, sell_percentage, data)\n",
    "    last_depot_value = data['depot_value'].iloc[-1]\n",
    "    return last_depot_value\n",
    "\n",
    "def broker(stock_value, money_value, buy_threshold, sell_threshold, sell_percentage, data):\n",
    "    stock_value = stock_value\n",
    "    money_value = money_value\n",
    "    depot_value = 0\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        stock_value *= row[\"price_change_1\"]\n",
    "        if row['next_day_percentage_predicted'] >= buy_threshold:\n",
    "            if money_value > 0:\n",
    "                money_value -= fee\n",
    "                stock_value += money_value\n",
    "                money_value = 0\n",
    "        elif row['next_day_percentage_predicted'] <= sell_threshold:\n",
    "                if stock_value > 0:\n",
    "                    money_value += stock_value * sell_percentage\n",
    "                    stock_value -= stock_value * sell_percentage\n",
    "                    money_value -= fee\n",
    "        depot_value = stock_value + money_value\n",
    "        data.at[index, 'depot_value'] = depot_value\n",
    "    return data[\"depot_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "for _ in range(iterations):\n",
    "    study.optimize(lambda trial: optuna_loop(trial, data, stock_value, money_value, sell_percentage), n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "buy_threshold = round(best_params['buy_threshold'],3)\n",
    "sell_threshold = round(best_params['sell_threshold'],3)\n",
    "stock_value = 1000\n",
    "money_value = 0\n",
    "logger(\"Buy threshold: \" + str(buy_threshold) + \"%, Sell threshold: \" + str(sell_threshold)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"buy_or_sell\"] = data[\"next_day_percentage_predicted\"].apply(lambda x: 1 if x >= buy_threshold else (-1 if x <= sell_threshold else 0))\n",
    "data[\"depot_value\"] = broker(stock_value, money_value, buy_threshold, sell_threshold, sell_percentage, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_percentage = round(((data['mean_price'].iloc[-1] / data['mean_price'].iloc[0]) - 1) * 100, 2)\n",
    "spai_percentage = round(((data['depot_value'].iloc[-1] / data['depot_value'].iloc[0]) - 1) * 100, 2)\n",
    "\n",
    "logger(\"Hold percentage: \" + str(hold_percentage) + \"%, SPAI percentage: \" + str(spai_percentage)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mean_price'] = data['mean_price'] / data['mean_price'].iloc[0]\n",
    "data['depot_value'] = data['depot_value'] / data['depot_value'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get last value of buy_or_sell\n",
    "today_buy_or_sell = data['buy_or_sell'].iloc[-1]\n",
    "today = data['date'].iloc[-1]\n",
    "\n",
    "if today_buy_or_sell == 1:\n",
    "    print(\"Buy signal today \", today)\n",
    "elif today_buy_or_sell == -1:\n",
    "    print(\"Sell signal today\", today)\n",
    "elif today_buy_or_sell == 0:\n",
    "    print(\"Hold signal today\", today)\n",
    "\n",
    "logger(\"Signal for today: \" + str(today_buy_or_sell) + \", Date: \" + str(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(data['date'], data[\"mean_price\"], label='Holding', marker='o', color='blue')\n",
    "plt.plot(data['date'], data[\"depot_value\"], label='Using SPAI', marker='x', color='purple')\n",
    "\n",
    "buy_label_added = False\n",
    "sell_label_added = False\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['buy_or_sell'] >= 1:\n",
    "        if not buy_label_added:\n",
    "            plt.bar(row['date'], height=0.1, bottom=row[\"mean_price\"] - 0.05, color='green', width=1, alpha=0.5, label =\"buy\")\n",
    "            buy_label_added = True\n",
    "        else:\n",
    "            plt.bar(row['date'], height=0.1, bottom=row[\"mean_price\"] - 0.05, color='green', width=1, alpha=0.5)\n",
    "    elif row['buy_or_sell'] <= -1:\n",
    "        if not sell_label_added:\n",
    "            plt.bar(row['date'], height=0.1, bottom=row[\"mean_price\"] - 0.05, color='red', width=1, alpha=0.5, label = \"sell\")\n",
    "            sell_label_added = True\n",
    "        else:\n",
    "            plt.bar(row['date'], height=0.1, bottom=row[\"mean_price\"] - 0.05, color='red', width=1, alpha=0.5)\n",
    "plt.title(title)\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('normierter Depotwert')\n",
    "plt.legend()\n",
    "plt.xticks(data['date'][::10], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Simulation.pdf', format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to Output folder\n",
    "data.to_csv('output/data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = pd.Timestamp.now()\n",
    "time_delta = time_end - time_start\n",
    "logger(\"Script ended: \" + str(time_start))\n",
    "logger(\"Script needed: \" + str(time_delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
